{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Celltype Classification with Transcriptformer\n",
    "\n",
    "This notebooks showcase how the Transcriptformer embeddings can be used to train a classifier for celltype classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "from transcriptformer.model.inference import run_inference\n",
    "from transcriptformer.datasets import tabula_sapiens\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174M/174M [00:06<00:00, 29.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "adata = tabula_sapiens(tissue=\"ear\", version=\"v2\")\n",
    "cfg = OmegaConf.load(\"./../conf/inference_config.yaml\")\n",
    "logging.debug(OmegaConf.to_yaml(cfg))\n",
    "\n",
    "cfg.model.checkpoint_path = \"./../checkpoints/tf_sapiens\"\n",
    "\n",
    "config_path = os.path.join(cfg.model.checkpoint_path, \"config.json\")\n",
    "with open(config_path) as f:\n",
    "    config_dict = json.load(f)\n",
    "mlflow_cfg = OmegaConf.create(config_dict)\n",
    "\n",
    "# Merge the MLflow config with the main config\n",
    "cfg = OmegaConf.merge(mlflow_cfg, cfg)\n",
    "\n",
    "cfg.model.inference_config.data_files = ['./../test/data/human_val.h5ad']\n",
    "\n",
    "# Set the checkpoint paths based on the unified checkpoint_path\n",
    "cfg.model.inference_config.load_checkpoint = os.path.join(cfg.model.checkpoint_path, \"model_weights.pt\")\n",
    "cfg.model.data_config.aux_vocab_path = os.path.join(cfg.model.checkpoint_path, \"vocabs\")\n",
    "cfg.model.data_config.esm2_mappings_path = os.path.join(cfg.model.checkpoint_path, \"vocabs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var[\"feature_id\"] = adata.var_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 17:30:44,189 - ERROR - Failed to read file AnnData object with n_obs × n_vars = 3055 × 21897\n",
      "    obs: 'donor_id', 'tissue_in_publication', 'anatomical_position', 'method', 'cdna_plate', 'library_plate', 'notes', 'cdna_well', 'assay_ontology_term_id', 'sample_id', 'replicate', '10X_run', 'ambient_removal', 'donor_method', 'donor_assay', 'donor_tissue', 'donor_tissue_assay', 'cell_type_ontology_term_id', 'compartment', 'broad_cell_class', 'free_annotation', 'manually_annotated', 'published_2022', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ercc', 'pct_counts_ercc', '_scvi_batch', '_scvi_labels', 'scvi_leiden_donorassay_full', 'ethnicity_original', 'scvi_leiden_res05_tissue', 'sample_number', 'organism_ontology_term_id', 'suspension_type', 'tissue_type', 'disease_ontology_term_id', 'is_primary_data', 'tissue_ontology_term_id', 'sex_ontology_term_id', 'self_reported_ethnicity_ontology_term_id', 'development_stage_ontology_term_id', 'cell_type', 'assay', 'disease', 'organism', 'sex', 'tissue', 'self_reported_ethnicity', 'development_stage', 'observation_joinid'\n",
      "    var: 'ensembl_id', 'genome', 'mt', 'ercc', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'mean', 'std', 'feature_is_filtered', 'feature_name', 'feature_reference', 'feature_biotype', 'feature_length', 'feature_type', 'feature_id'\n",
      "    uns: '_scvi_manager_uuid', '_scvi_uuid', '_training_mode', 'assay_ontology_term_id_colors', 'citation', 'compartment_colors', 'donor_id_colors', 'leiden', 'method_colors', 'neighbors', 'pca', 'schema_reference', 'schema_version', 'sex_ontology_term_id_colors', 'tissue_in_publication_colors', 'title', 'umap'\n",
      "    obsm: 'X_pca', 'X_scvi', 'X_tissue_uncorrected_umap', 'X_umap', 'X_umap_scvi_full_donorassay', 'X_umap_tissue_scvi_donorassay', 'X_uncorrected_umap'\n",
      "    varm: 'PCs'\n",
      "    layers: 'decontXcounts', 'scale_data'\n",
      "    obsp: 'connectivities', 'distances': expected str, bytes or os.PathLike object, not AnnData\n",
      "2025-04-15 17:30:44,190 - ERROR - Failed to load data from AnnData object with n_obs × n_vars = 3055 × 21897\n",
      "    obs: 'donor_id', 'tissue_in_publication', 'anatomical_position', 'method', 'cdna_plate', 'library_plate', 'notes', 'cdna_well', 'assay_ontology_term_id', 'sample_id', 'replicate', '10X_run', 'ambient_removal', 'donor_method', 'donor_assay', 'donor_tissue', 'donor_tissue_assay', 'cell_type_ontology_term_id', 'compartment', 'broad_cell_class', 'free_annotation', 'manually_annotated', 'published_2022', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'total_counts_ercc', 'pct_counts_ercc', '_scvi_batch', '_scvi_labels', 'scvi_leiden_donorassay_full', 'ethnicity_original', 'scvi_leiden_res05_tissue', 'sample_number', 'organism_ontology_term_id', 'suspension_type', 'tissue_type', 'disease_ontology_term_id', 'is_primary_data', 'tissue_ontology_term_id', 'sex_ontology_term_id', 'self_reported_ethnicity_ontology_term_id', 'development_stage_ontology_term_id', 'cell_type', 'assay', 'disease', 'organism', 'sex', 'tissue', 'self_reported_ethnicity', 'development_stage', 'observation_joinid'\n",
      "    var: 'ensembl_id', 'genome', 'mt', 'ercc', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'mean', 'std', 'feature_is_filtered', 'feature_name', 'feature_reference', 'feature_biotype', 'feature_length', 'feature_type', 'feature_id'\n",
      "    uns: '_scvi_manager_uuid', '_scvi_uuid', '_training_mode', 'assay_ontology_term_id_colors', 'citation', 'compartment_colors', 'donor_id_colors', 'leiden', 'method_colors', 'neighbors', 'pca', 'schema_reference', 'schema_version', 'sex_ontology_term_id_colors', 'tissue_in_publication_colors', 'title', 'umap'\n",
      "    obsm: 'X_pca', 'X_scvi', 'X_tissue_uncorrected_umap', 'X_umap', 'X_umap_scvi_full_donorassay', 'X_umap_tissue_scvi_donorassay', 'X_uncorrected_umap'\n",
      "    varm: 'PCs'\n",
      "    layers: 'decontXcounts', 'scale_data'\n",
      "    obsp: 'connectivities', 'distances'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid data was loaded from any files. Check if files exist and contain valid data after filtering.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Set logging level to ERROR to reduce verbosity\u001b[39;00m\n\u001b[32m      2\u001b[39m logging.getLogger().setLevel(logging.ERROR)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m adata_output = \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43madata\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/src/transcriptformer/model/inference.py:93\u001b[39m, in \u001b[36mrun_inference\u001b[39m\u001b[34m(cfg, data_files)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m     77\u001b[39m data_kwargs = {\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgene_vocab\u001b[39m\u001b[33m\"\u001b[39m: gene_vocab,\n\u001b[32m     79\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maux_vocab\u001b[39m\u001b[33m\"\u001b[39m: aux_vocab,\n\u001b[32m   (...)\u001b[39m\u001b[32m     91\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mobs_keys\u001b[39m\u001b[33m\"\u001b[39m: cfg.model.inference_config.obs_keys,\n\u001b[32m     92\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m dataset = \u001b[43mAnnDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Create dataloader\u001b[39;00m\n\u001b[32m     96\u001b[39m dataloader = DataLoader(\n\u001b[32m     97\u001b[39m     dataset,\n\u001b[32m     98\u001b[39m     batch_size=cfg.model.inference_config.batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m     collate_fn=dataset.collate_fn,\n\u001b[32m    103\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/src/transcriptformer/data/dataloader.py:235\u001b[39m, in \u001b[36mAnnDataset.__init__\u001b[39m\u001b[34m(self, files_list, gene_vocab, data_dir, aux_vocab, max_len, normalize_to_scale, sort_genes, randomize_order, pad_zeros, gene_col_name, filter_to_vocab, filter_outliers, min_expressed_genes, seed, pad_token, clip_counts, inference, obs_keys)\u001b[39m\n\u001b[32m    232\u001b[39m random.seed(\u001b[38;5;28mself\u001b[39m.seed)\n\u001b[32m    234\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[33mLoading and processing all data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_and_process_all_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/work/src/transcriptformer/data/dataloader.py:318\u001b[39m, in \u001b[36mAnnDataset.load_and_process_all_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    316\u001b[39m \u001b[38;5;66;03m# Add check for empty all_data list\u001b[39;00m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_data:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    319\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo valid data was loaded from any files. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    320\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCheck if files exist and contain valid data after filtering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m     )\n\u001b[32m    323\u001b[39m concatenated_batch = BatchData(\n\u001b[32m    324\u001b[39m     gene_counts=torch.concat([batch.gene_counts \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m all_data]),\n\u001b[32m    325\u001b[39m     gene_token_indices=torch.concat([batch.gene_token_indices \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m all_data]),\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m     ),\n\u001b[32m    337\u001b[39m )\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m concatenated_batch\n",
      "\u001b[31mValueError\u001b[39m: No valid data was loaded from any files. Check if files exist and contain valid data after filtering."
     ]
    }
   ],
   "source": [
    "# Set logging level to ERROR to reduce verbosity\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "adata_output = run_inference(cfg, data_files=[adata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
